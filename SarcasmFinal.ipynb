{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sarcasm Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package stopwords to /home/jason/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jason/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import nps_chat\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import pandas as pd \n",
    "import re\n",
    "import pickle\n",
    "import datetime\n",
    "import io\n",
    "\n",
    "\n",
    "import os\n",
    "from os import makedirs\n",
    "from os import chdir\n",
    "from os import path\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENdata = pd.read_csv(cwd + '/sarcasmCorpora/GEN-sarc-notsarc.csv')\n",
    "HYPdata = pd.read_csv(cwd + '/sarcasmCorpora/HYP-sarc-notsarc.csv')\n",
    "RQdata = pd.read_csv(cwd + '/sarcasmCorpora/RQ-sarc-notsarc.csv')\n",
    "\n",
    "GENdata['Type'] = 'general' \n",
    "HYPdata['Type'] = 'hyperbole'\n",
    "RQdata['Type'] = 'rhetorical'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(text, labels, split_size):\n",
    "    sentences_train, sentences_test, label_train, label_test = train_test_split(\n",
    "        text, labels, test_size = split_size, random_state = 42)\n",
    "    \n",
    "    sentences_train = np.array(sentences_train)\n",
    "    label_train = np.array(label_train)\n",
    "    sentences_test = np.array(sentences_test)\n",
    "    label_test = np.array(label_test)\n",
    "    \n",
    "    return sentences_train, sentences_test, label_train, label_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding Sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 200\n",
    "\n",
    "def make_sequences(tok, train_text, test_text):\n",
    "    training_sequences = tok.texts_to_sequences(train_text)\n",
    "    training_padded = pad_sequences(training_sequences, maxlen=max_length, \n",
    "                                    padding = 'post', truncating = 'post')\n",
    "    \n",
    "    testing_sequences = tok.texts_to_sequences(test_text)\n",
    "    testing_padded = pad_sequences(testing_sequences, maxlen=max_length, \n",
    "                                   padding = 'post', truncating = 'post')\n",
    "    \n",
    "    return training_padded, testing_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"\\?\", \"\", text)\n",
    "    text = re.sub(r\"\\!\", \"\", text)\n",
    "    text = re.sub(r\"\\.\", \"\", text)\n",
    "    text = re.sub(r\"\\,\", \"\", text)\n",
    "    text = re.sub(r\"\\r\\n\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN1: Type of input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPdata['id'] = HYPdata['id'].apply(lambda x: x + 6520)\n",
    "RQdata['id'] = RQdata['id'].apply(lambda x: x + 6520 + 1164)\n",
    "\n",
    "sarcasm_dataset = GENdata.append(HYPdata).append(RQdata).set_index('id')\n",
    "\n",
    "type_labels = sarcasm_dataset['Type'].tolist()\n",
    "sentencesRaw = sarcasm_dataset['text'].tolist()\n",
    "sentences = list(map(clean_text, sentencesRaw))\n",
    "\n",
    "Type_test_size = 0.5\n",
    "\n",
    "Type_train, Type_test, Type_label_train, Type_label_test = split_data(sentences, type_labels, Type_test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_Type_train, nl_Type_test = [],[]\n",
    "\n",
    "def number_label(l, new_l):\n",
    "    for element in l:\n",
    "        if(element == 'general'):\n",
    "            new_l.append(0)\n",
    "        elif(element == 'hyperbole'):\n",
    "            new_l.append(1)\n",
    "        elif(element == 'rhetorical'):\n",
    "            new_l.append(2)\n",
    "            \n",
    "number_label(Type_label_train, nl_Type_train)\n",
    "number_label(Type_label_test, nl_Type_test)\n",
    "\n",
    "nl_Type_train, nl_Type_test = np.array(nl_Type_train), np.array(nl_Type_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Type_tok = Tokenizer(oov_token=\"<OOV>\")\n",
    "Type_tok.fit_on_texts(Type_train)\n",
    "Type_word_index = Type_tok.word_index\n",
    "Type_vocab_size = len(Type_word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Type_train_padded, Type_test_padded = make_sequences(Type_tok, Type_train, Type_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4693, 200)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Type_train_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "TYPEembed (Embedding)        (None, 200, 32)           573920    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 24)                792       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 75        \n",
      "=================================================================\n",
      "Total params: 574,787\n",
      "Trainable params: 574,787\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "147/147 - 2s - loss: 0.9754 - accuracy: 0.6648 - val_loss: 0.7991 - val_accuracy: 0.7057\n",
      "Epoch 2/30\n",
      "147/147 - 1s - loss: 0.8163 - accuracy: 0.6836 - val_loss: 0.7784 - val_accuracy: 0.7057\n",
      "Epoch 3/30\n",
      "147/147 - 2s - loss: 0.8052 - accuracy: 0.6836 - val_loss: 0.7708 - val_accuracy: 0.7057\n",
      "Epoch 4/30\n",
      "147/147 - 2s - loss: 0.7905 - accuracy: 0.6844 - val_loss: 0.7626 - val_accuracy: 0.7049\n",
      "Epoch 5/30\n",
      "147/147 - 2s - loss: 0.7741 - accuracy: 0.6910 - val_loss: 0.7559 - val_accuracy: 0.7094\n",
      "Epoch 6/30\n",
      "147/147 - 1s - loss: 0.7516 - accuracy: 0.7023 - val_loss: 0.7595 - val_accuracy: 0.7079\n",
      "Epoch 7/30\n",
      "147/147 - 1s - loss: 0.7182 - accuracy: 0.7128 - val_loss: 0.7459 - val_accuracy: 0.7064\n",
      "Epoch 8/30\n",
      "147/147 - 1s - loss: 0.6687 - accuracy: 0.7311 - val_loss: 0.7375 - val_accuracy: 0.7040\n",
      "Epoch 9/30\n",
      "147/147 - 1s - loss: 0.6068 - accuracy: 0.7496 - val_loss: 0.7393 - val_accuracy: 0.7008\n",
      "Epoch 10/30\n",
      "147/147 - 1s - loss: 0.5462 - accuracy: 0.7633 - val_loss: 0.7547 - val_accuracy: 0.7021\n",
      "Epoch 11/30\n",
      "147/147 - 1s - loss: 0.4887 - accuracy: 0.7831 - val_loss: 0.7622 - val_accuracy: 0.7153\n",
      "Epoch 12/30\n",
      "147/147 - 1s - loss: 0.4424 - accuracy: 0.8118 - val_loss: 0.7731 - val_accuracy: 0.7053\n",
      "Epoch 13/30\n",
      "147/147 - 1s - loss: 0.4001 - accuracy: 0.8378 - val_loss: 0.7948 - val_accuracy: 0.7183\n",
      "Epoch 14/30\n",
      "147/147 - 1s - loss: 0.3648 - accuracy: 0.8602 - val_loss: 0.7997 - val_accuracy: 0.7108\n",
      "Epoch 15/30\n",
      "147/147 - 1s - loss: 0.3311 - accuracy: 0.8771 - val_loss: 0.8193 - val_accuracy: 0.6921\n",
      "Epoch 16/30\n",
      "147/147 - 1s - loss: 0.3061 - accuracy: 0.8935 - val_loss: 0.8358 - val_accuracy: 0.6951\n",
      "Epoch 17/30\n",
      "147/147 - 1s - loss: 0.2774 - accuracy: 0.9073 - val_loss: 0.8632 - val_accuracy: 0.7113\n",
      "Epoch 18/30\n",
      "147/147 - 1s - loss: 0.2533 - accuracy: 0.9180 - val_loss: 0.8653 - val_accuracy: 0.7055\n",
      "Epoch 19/30\n",
      "147/147 - 1s - loss: 0.2318 - accuracy: 0.9267 - val_loss: 0.8885 - val_accuracy: 0.6993\n",
      "Epoch 20/30\n",
      "147/147 - 1s - loss: 0.2127 - accuracy: 0.9325 - val_loss: 0.9095 - val_accuracy: 0.7149\n",
      "Epoch 21/30\n",
      "147/147 - 1s - loss: 0.1931 - accuracy: 0.9414 - val_loss: 0.9198 - val_accuracy: 0.7000\n",
      "Epoch 22/30\n",
      "147/147 - 1s - loss: 0.1807 - accuracy: 0.9397 - val_loss: 0.9327 - val_accuracy: 0.7057\n",
      "Epoch 23/30\n",
      "147/147 - 1s - loss: 0.1656 - accuracy: 0.9484 - val_loss: 0.9762 - val_accuracy: 0.7091\n",
      "Epoch 24/30\n",
      "147/147 - 1s - loss: 0.1565 - accuracy: 0.9514 - val_loss: 0.9736 - val_accuracy: 0.7130\n",
      "Epoch 25/30\n",
      "147/147 - 1s - loss: 0.1422 - accuracy: 0.9525 - val_loss: 0.9914 - val_accuracy: 0.7115\n",
      "Epoch 26/30\n",
      "147/147 - 1s - loss: 0.1321 - accuracy: 0.9587 - val_loss: 1.0151 - val_accuracy: 0.7132\n",
      "Epoch 27/30\n",
      "147/147 - 1s - loss: 0.1247 - accuracy: 0.9591 - val_loss: 1.0473 - val_accuracy: 0.7059\n",
      "Epoch 28/30\n",
      "147/147 - 1s - loss: 0.1165 - accuracy: 0.9595 - val_loss: 1.0627 - val_accuracy: 0.7081\n",
      "Epoch 29/30\n",
      "147/147 - 1s - loss: 0.1091 - accuracy: 0.9599 - val_loss: 1.0665 - val_accuracy: 0.6996\n",
      "Epoch 30/30\n",
      "147/147 - 1s - loss: 0.1045 - accuracy: 0.9629 - val_loss: 1.0847 - val_accuracy: 0.7019\n"
     ]
    }
   ],
   "source": [
    "Type_embedding_dim = 32\n",
    "\n",
    "Type_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(Type_vocab_size, Type_embedding_dim, input_length = max_length, name = 'TYPEembed'),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(24, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(3, activation = 'sigmoid'),\n",
    "    ])\n",
    "\n",
    "Type_model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "Type_model.summary()\n",
    "\n",
    "Type_history = Type_model.fit(Type_train_padded, nl_Type_train, epochs = 30, \n",
    "                              validation_data = (Type_test_padded, nl_Type_test), verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN2: Sarcasm or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEN_text = list(map(clean_text, GENdata['text'].tolist()))\n",
    "GEN_labels = GENdata['class'].tolist()\n",
    "HYP_text = list(map(clean_text, HYPdata['text'].tolist()))\n",
    "HYP_labels = HYPdata['class'].tolist()\n",
    "RQ_text = list(map(clean_text, RQdata['text'].tolist()))\n",
    "RQ_labels = RQdata['class'].tolist()\n",
    "\n",
    "GEN_test_size, HYP_test_size, RQ_test_size = 0.33, 0.1, 0.4\n",
    "\n",
    "GEN_train, GEN_test, GEN_label_train, GEN_label_test = split_data(GEN_text, GEN_labels, GEN_test_size)\n",
    "HYP_train, HYP_test, HYP_label_train, HYP_label_test = split_data(HYP_text, HYP_labels, HYP_test_size)\n",
    "RQ_train, RQ_test, RQ_label_train, RQ_label_test = split_data(RQ_text, RQ_labels, RQ_test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "blGEN_train, blGEN_test, blHYP_train, blHYP_test, blRQ_train, blRQ_test = [],[],[],[],[],[]\n",
    "\n",
    "def make_binery(l, new_l):\n",
    "    for element in l:\n",
    "        if(element == 'notsarc'):\n",
    "            new_l.append(0)\n",
    "        elif(element == 'sarc'):\n",
    "            new_l.append(1)\n",
    "            \n",
    "    new_l = np.array(new_l)\n",
    "            \n",
    "make_binery(GEN_label_train, blGEN_train)\n",
    "make_binery(GEN_label_test, blGEN_test)\n",
    "make_binery(HYP_label_train, blHYP_train)\n",
    "make_binery(HYP_label_test, blHYP_test)\n",
    "make_binery(RQ_label_train, blRQ_train)\n",
    "make_binery(RQ_label_test, blRQ_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "blGEN_train, blGEN_test = np.array(blGEN_train), np.array(blGEN_test)\n",
    "blHYP_train, blHYP_test = np.array(blHYP_train), np.array(blHYP_test)\n",
    "blRQ_train, blRQ_test = np.array(blRQ_train), np.array(blRQ_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENtok = Tokenizer(oov_token=\"<OOV>\")\n",
    "GENtok.fit_on_texts(GEN_train)\n",
    "GEN_word_index = GENtok.word_index\n",
    "GEN_vocab_size = len(GEN_word_index) + 1\n",
    "\n",
    "HYPtok = Tokenizer(oov_token=\"<OOV>\")\n",
    "HYPtok.fit_on_texts(HYP_train)\n",
    "HYP_word_index = HYPtok.word_index\n",
    "HYP_vocab_size = len(HYP_word_index) + 1\n",
    "\n",
    "RQtok = Tokenizer(oov_token=\"<OOV>\")\n",
    "RQtok.fit_on_texts(RQ_train)\n",
    "RQ_word_index = RQtok.word_index\n",
    "RQ_vocab_size = len(RQ_word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEN_train_padded, GEN_test_padded = make_sequences(GENtok, GEN_train, GEN_test)\n",
    "HYP_train_padded, HYP_test_padded = make_sequences(HYPtok, HYP_train, HYP_test)\n",
    "RQ_train_padded, RQ_test_padded = make_sequences(RQtok, RQ_train, RQ_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "GENembed (Embedding)         (None, 200, 16)           259760    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 24)                408       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                800       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 12)                396       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 261,649\n",
      "Trainable params: 261,649\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "137/137 - 1s - loss: 0.6945 - accuracy: 0.5050 - val_loss: 0.6928 - val_accuracy: 0.5000\n",
      "Epoch 2/15\n",
      "137/137 - 1s - loss: 0.6924 - accuracy: 0.5142 - val_loss: 0.6906 - val_accuracy: 0.5214\n",
      "Epoch 3/15\n",
      "137/137 - 1s - loss: 0.6859 - accuracy: 0.5721 - val_loss: 0.6759 - val_accuracy: 0.6394\n",
      "Epoch 4/15\n",
      "137/137 - 1s - loss: 0.6583 - accuracy: 0.6467 - val_loss: 0.6385 - val_accuracy: 0.6589\n",
      "Epoch 5/15\n",
      "137/137 - 1s - loss: 0.6223 - accuracy: 0.6651 - val_loss: 0.6144 - val_accuracy: 0.6617\n",
      "Epoch 6/15\n",
      "137/137 - 1s - loss: 0.6032 - accuracy: 0.6788 - val_loss: 0.6055 - val_accuracy: 0.6687\n",
      "Epoch 7/15\n",
      "137/137 - 1s - loss: 0.5836 - accuracy: 0.6983 - val_loss: 0.5917 - val_accuracy: 0.6914\n",
      "Epoch 8/15\n",
      "137/137 - 1s - loss: 0.5498 - accuracy: 0.7202 - val_loss: 0.5719 - val_accuracy: 0.7063\n",
      "Epoch 9/15\n",
      "137/137 - 1s - loss: 0.5061 - accuracy: 0.7571 - val_loss: 0.5834 - val_accuracy: 0.6887\n",
      "Epoch 10/15\n",
      "137/137 - 1s - loss: 0.4548 - accuracy: 0.7924 - val_loss: 0.5742 - val_accuracy: 0.7124\n",
      "Epoch 11/15\n",
      "137/137 - 1s - loss: 0.4120 - accuracy: 0.8235 - val_loss: 0.5504 - val_accuracy: 0.7347\n",
      "Epoch 12/15\n",
      "137/137 - 1s - loss: 0.3613 - accuracy: 0.8484 - val_loss: 0.5601 - val_accuracy: 0.7356\n",
      "Epoch 13/15\n",
      "137/137 - 1s - loss: 0.3171 - accuracy: 0.8755 - val_loss: 0.6030 - val_accuracy: 0.7175\n",
      "Epoch 14/15\n",
      "137/137 - 1s - loss: 0.2766 - accuracy: 0.8910 - val_loss: 0.6104 - val_accuracy: 0.7244\n",
      "Epoch 15/15\n",
      "137/137 - 1s - loss: 0.2430 - accuracy: 0.9061 - val_loss: 0.6266 - val_accuracy: 0.7300\n"
     ]
    }
   ],
   "source": [
    "GEN_embedding_dim = 16\n",
    "\n",
    "GEN_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(GEN_vocab_size, GEN_embedding_dim, \n",
    "                                  input_length = max_length, name = 'GENembed'),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(16, activation = 'sigmoid'),\n",
    "        tf.keras.layers.Dense(24, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(32, activation = 'sigmoid'),\n",
    "        tf.keras.layers.Dense(12, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "    ])\n",
    "\n",
    "GEN_model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "GEN_model.summary()\n",
    "GEN_n_epochs = 15\n",
    "\n",
    "GEN_history = GEN_model.fit(GEN_train_padded, blGEN_train, epochs = GEN_n_epochs, \n",
    "                            validation_data = (GEN_test_padded, blGEN_test), verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperbolic NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "HYPembed (Embedding)         (None, 200, 32)           250528    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 251,345\n",
      "Trainable params: 251,345\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "33/33 - 0s - loss: 0.7254 - accuracy: 0.5033 - val_loss: 0.7222 - val_accuracy: 0.4530\n",
      "Epoch 2/20\n",
      "33/33 - 0s - loss: 0.7175 - accuracy: 0.5186 - val_loss: 0.7153 - val_accuracy: 0.4530\n",
      "Epoch 3/20\n",
      "33/33 - 0s - loss: 0.7113 - accuracy: 0.5578 - val_loss: 0.7107 - val_accuracy: 0.4615\n",
      "Epoch 4/20\n",
      "33/33 - 0s - loss: 0.7057 - accuracy: 0.5330 - val_loss: 0.7061 - val_accuracy: 0.5214\n",
      "Epoch 5/20\n",
      "33/33 - 0s - loss: 0.7008 - accuracy: 0.6074 - val_loss: 0.7023 - val_accuracy: 0.5470\n",
      "Epoch 6/20\n",
      "33/33 - 0s - loss: 0.6938 - accuracy: 0.6609 - val_loss: 0.7006 - val_accuracy: 0.5299\n",
      "Epoch 7/20\n",
      "33/33 - 0s - loss: 0.6820 - accuracy: 0.6590 - val_loss: 0.6906 - val_accuracy: 0.6581\n",
      "Epoch 8/20\n",
      "33/33 - 0s - loss: 0.6603 - accuracy: 0.7794 - val_loss: 0.6810 - val_accuracy: 0.6667\n",
      "Epoch 9/20\n",
      "33/33 - 0s - loss: 0.6162 - accuracy: 0.7975 - val_loss: 0.6738 - val_accuracy: 0.6410\n",
      "Epoch 10/20\n",
      "33/33 - 0s - loss: 0.5481 - accuracy: 0.8606 - val_loss: 0.6479 - val_accuracy: 0.6752\n",
      "Epoch 11/20\n",
      "33/33 - 0s - loss: 0.4610 - accuracy: 0.9112 - val_loss: 0.6527 - val_accuracy: 0.6667\n",
      "Epoch 12/20\n",
      "33/33 - 0s - loss: 0.3894 - accuracy: 0.9083 - val_loss: 0.6511 - val_accuracy: 0.6752\n",
      "Epoch 13/20\n",
      "33/33 - 0s - loss: 0.3318 - accuracy: 0.9179 - val_loss: 0.6437 - val_accuracy: 0.6581\n",
      "Epoch 14/20\n",
      "33/33 - 0s - loss: 0.2796 - accuracy: 0.9475 - val_loss: 0.6608 - val_accuracy: 0.6667\n",
      "Epoch 15/20\n",
      "33/33 - 0s - loss: 0.2460 - accuracy: 0.9561 - val_loss: 0.6840 - val_accuracy: 0.6581\n",
      "Epoch 16/20\n",
      "33/33 - 0s - loss: 0.2191 - accuracy: 0.9694 - val_loss: 0.7036 - val_accuracy: 0.6496\n",
      "Epoch 17/20\n",
      "33/33 - 0s - loss: 0.1993 - accuracy: 0.9733 - val_loss: 0.7204 - val_accuracy: 0.6325\n",
      "Epoch 18/20\n",
      "33/33 - 0s - loss: 0.1805 - accuracy: 0.9752 - val_loss: 0.7338 - val_accuracy: 0.6410\n",
      "Epoch 19/20\n",
      "33/33 - 0s - loss: 0.1682 - accuracy: 0.9809 - val_loss: 0.7627 - val_accuracy: 0.6239\n",
      "Epoch 20/20\n",
      "33/33 - 0s - loss: 0.1618 - accuracy: 0.9809 - val_loss: 0.7812 - val_accuracy: 0.6239\n"
     ]
    }
   ],
   "source": [
    "HYP_embedding_dim = 32\n",
    "\n",
    "HYP_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(HYP_vocab_size, HYP_embedding_dim, \n",
    "                                  input_length = max_length, name = 'HYPembed'),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001), activation = 'relu'),\n",
    "        tf.keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001), activation = 'relu'),\n",
    "        tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "    ])\n",
    "\n",
    "HYP_model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "HYP_model.summary()\n",
    "HYP_n_epochs = 20\n",
    "\n",
    "HYP_history = HYP_model.fit(HYP_train_padded, blHYP_train, epochs = HYP_n_epochs, \n",
    "                            validation_data = (HYP_test_padded, blHYP_test), verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rhetorical NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "RQembed (Embedding)          (None, 200, 3)            26796     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_3 ( (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 27,149\n",
      "Trainable params: 27,149\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "32/32 - 0s - loss: 0.7145 - accuracy: 0.5044 - val_loss: 0.7123 - val_accuracy: 0.4934\n",
      "Epoch 2/50\n",
      "32/32 - 0s - loss: 0.7101 - accuracy: 0.5044 - val_loss: 0.7085 - val_accuracy: 0.4934\n",
      "Epoch 3/50\n",
      "32/32 - 0s - loss: 0.7067 - accuracy: 0.5044 - val_loss: 0.7056 - val_accuracy: 0.4934\n",
      "Epoch 4/50\n",
      "32/32 - 0s - loss: 0.7042 - accuracy: 0.5162 - val_loss: 0.7029 - val_accuracy: 0.5727\n",
      "Epoch 5/50\n",
      "32/32 - 0s - loss: 0.7011 - accuracy: 0.5690 - val_loss: 0.7010 - val_accuracy: 0.5081\n",
      "Epoch 6/50\n",
      "32/32 - 0s - loss: 0.6991 - accuracy: 0.5328 - val_loss: 0.6994 - val_accuracy: 0.4978\n",
      "Epoch 7/50\n",
      "32/32 - 0s - loss: 0.6969 - accuracy: 0.5544 - val_loss: 0.6978 - val_accuracy: 0.5154\n",
      "Epoch 8/50\n",
      "32/32 - 0s - loss: 0.6946 - accuracy: 0.5808 - val_loss: 0.6965 - val_accuracy: 0.5140\n",
      "Epoch 9/50\n",
      "32/32 - 0s - loss: 0.6922 - accuracy: 0.5690 - val_loss: 0.6950 - val_accuracy: 0.5228\n",
      "Epoch 10/50\n",
      "32/32 - 0s - loss: 0.6886 - accuracy: 0.6121 - val_loss: 0.6935 - val_accuracy: 0.5242\n",
      "Epoch 11/50\n",
      "32/32 - 0s - loss: 0.6831 - accuracy: 0.6141 - val_loss: 0.6900 - val_accuracy: 0.5903\n",
      "Epoch 12/50\n",
      "32/32 - 0s - loss: 0.6741 - accuracy: 0.6611 - val_loss: 0.6889 - val_accuracy: 0.5404\n",
      "Epoch 13/50\n",
      "32/32 - 0s - loss: 0.6591 - accuracy: 0.6807 - val_loss: 0.6809 - val_accuracy: 0.5991\n",
      "Epoch 14/50\n",
      "32/32 - 0s - loss: 0.6350 - accuracy: 0.7258 - val_loss: 0.6773 - val_accuracy: 0.6079\n",
      "Epoch 15/50\n",
      "32/32 - 0s - loss: 0.6007 - accuracy: 0.7884 - val_loss: 0.6683 - val_accuracy: 0.6123\n",
      "Epoch 16/50\n",
      "32/32 - 0s - loss: 0.5533 - accuracy: 0.8462 - val_loss: 0.6555 - val_accuracy: 0.6461\n",
      "Epoch 17/50\n",
      "32/32 - 0s - loss: 0.5002 - accuracy: 0.8727 - val_loss: 0.6423 - val_accuracy: 0.6887\n",
      "Epoch 18/50\n",
      "32/32 - 0s - loss: 0.4391 - accuracy: 0.9079 - val_loss: 0.6375 - val_accuracy: 0.6916\n",
      "Epoch 19/50\n",
      "32/32 - 0s - loss: 0.3868 - accuracy: 0.9128 - val_loss: 0.6574 - val_accuracy: 0.6564\n",
      "Epoch 20/50\n",
      "32/32 - 0s - loss: 0.3367 - accuracy: 0.9422 - val_loss: 0.6420 - val_accuracy: 0.6858\n",
      "Epoch 21/50\n",
      "32/32 - 0s - loss: 0.2943 - accuracy: 0.9500 - val_loss: 0.6497 - val_accuracy: 0.6843\n",
      "Epoch 22/50\n",
      "32/32 - 0s - loss: 0.2600 - accuracy: 0.9618 - val_loss: 0.6466 - val_accuracy: 0.6814\n",
      "Epoch 23/50\n",
      "32/32 - 0s - loss: 0.2410 - accuracy: 0.9628 - val_loss: 0.6550 - val_accuracy: 0.6784\n",
      "Epoch 24/50\n",
      "32/32 - 0s - loss: 0.2110 - accuracy: 0.9804 - val_loss: 0.6714 - val_accuracy: 0.6858\n",
      "Epoch 25/50\n",
      "32/32 - 0s - loss: 0.1927 - accuracy: 0.9843 - val_loss: 0.6756 - val_accuracy: 0.6843\n",
      "Epoch 26/50\n",
      "32/32 - 0s - loss: 0.1774 - accuracy: 0.9912 - val_loss: 0.6882 - val_accuracy: 0.6872\n",
      "Epoch 27/50\n",
      "32/32 - 0s - loss: 0.1660 - accuracy: 0.9882 - val_loss: 0.6969 - val_accuracy: 0.6814\n",
      "Epoch 28/50\n",
      "32/32 - 0s - loss: 0.1556 - accuracy: 0.9922 - val_loss: 0.7134 - val_accuracy: 0.6872\n",
      "Epoch 29/50\n",
      "32/32 - 0s - loss: 0.1471 - accuracy: 0.9902 - val_loss: 0.7203 - val_accuracy: 0.6872\n",
      "Epoch 30/50\n",
      "32/32 - 0s - loss: 0.1403 - accuracy: 0.9931 - val_loss: 0.7293 - val_accuracy: 0.6858\n",
      "Epoch 31/50\n",
      "32/32 - 0s - loss: 0.1370 - accuracy: 0.9961 - val_loss: 0.7353 - val_accuracy: 0.6814\n",
      "Epoch 32/50\n",
      "32/32 - 0s - loss: 0.1295 - accuracy: 0.9951 - val_loss: 0.7431 - val_accuracy: 0.6799\n",
      "Epoch 33/50\n",
      "32/32 - 0s - loss: 0.1258 - accuracy: 0.9961 - val_loss: 0.7559 - val_accuracy: 0.6858\n",
      "Epoch 34/50\n",
      "32/32 - 0s - loss: 0.1216 - accuracy: 0.9971 - val_loss: 0.7575 - val_accuracy: 0.6799\n",
      "Epoch 35/50\n",
      "32/32 - 0s - loss: 0.1183 - accuracy: 0.9980 - val_loss: 0.7664 - val_accuracy: 0.6828\n",
      "Epoch 36/50\n",
      "32/32 - 0s - loss: 0.1154 - accuracy: 0.9980 - val_loss: 0.7700 - val_accuracy: 0.6799\n",
      "Epoch 37/50\n",
      "32/32 - 0s - loss: 0.1130 - accuracy: 0.9990 - val_loss: 0.7742 - val_accuracy: 0.6769\n",
      "Epoch 38/50\n",
      "32/32 - 0s - loss: 0.1106 - accuracy: 0.9990 - val_loss: 0.7843 - val_accuracy: 0.6828\n",
      "Epoch 39/50\n",
      "32/32 - 0s - loss: 0.1091 - accuracy: 0.9990 - val_loss: 0.7839 - val_accuracy: 0.6828\n",
      "Epoch 40/50\n",
      "32/32 - 0s - loss: 0.1062 - accuracy: 0.9990 - val_loss: 0.7893 - val_accuracy: 0.6814\n",
      "Epoch 41/50\n",
      "32/32 - 0s - loss: 0.1060 - accuracy: 0.9980 - val_loss: 0.7955 - val_accuracy: 0.6784\n",
      "Epoch 42/50\n",
      "32/32 - 0s - loss: 0.1029 - accuracy: 1.0000 - val_loss: 0.7934 - val_accuracy: 0.6769\n",
      "Epoch 43/50\n",
      "32/32 - 0s - loss: 0.1012 - accuracy: 1.0000 - val_loss: 0.7957 - val_accuracy: 0.6828\n",
      "Epoch 44/50\n",
      "32/32 - 0s - loss: 0.1000 - accuracy: 1.0000 - val_loss: 0.8085 - val_accuracy: 0.6858\n",
      "Epoch 45/50\n",
      "32/32 - 0s - loss: 0.0992 - accuracy: 1.0000 - val_loss: 0.8094 - val_accuracy: 0.6843\n",
      "Epoch 46/50\n",
      "32/32 - 0s - loss: 0.0968 - accuracy: 1.0000 - val_loss: 0.8050 - val_accuracy: 0.6769\n",
      "Epoch 47/50\n",
      "32/32 - 0s - loss: 0.0954 - accuracy: 1.0000 - val_loss: 0.8039 - val_accuracy: 0.6843\n",
      "Epoch 48/50\n",
      "32/32 - 0s - loss: 0.0939 - accuracy: 1.0000 - val_loss: 0.8069 - val_accuracy: 0.6784\n",
      "Epoch 49/50\n",
      "32/32 - 0s - loss: 0.0930 - accuracy: 1.0000 - val_loss: 0.8069 - val_accuracy: 0.6784\n",
      "Epoch 50/50\n",
      "32/32 - 0s - loss: 0.0916 - accuracy: 1.0000 - val_loss: 0.8079 - val_accuracy: 0.6814\n"
     ]
    }
   ],
   "source": [
    "RQ_embedding_dim = 3\n",
    "\n",
    "RQ_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(RQ_vocab_size, RQ_embedding_dim, input_length = max_length, name = 'RQembed'),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001), activation = 'relu'),\n",
    "        tf.keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001), activation = 'relu'), \n",
    "        tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "    ])\n",
    "\n",
    "RQ_model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "RQ_model.summary()\n",
    "RQ_n_epochs = 50\n",
    "\n",
    "RQ_history = RQ_model.fit(RQ_train_padded, blRQ_train, epochs = RQ_n_epochs, \n",
    "                          validation_data = (RQ_test_padded, blRQ_test), verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./logs/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:From /home/jason/.local/lib/python3.6/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0340s vs `on_train_batch_end` time: 0.2271s). Check your callbacks.\n",
      "32/32 - 0s - loss: 0.0909 - accuracy: 1.0000 - val_loss: 0.8115 - val_accuracy: 0.6755\n",
      "Epoch 2/50\n",
      "32/32 - 0s - loss: 0.0894 - accuracy: 1.0000 - val_loss: 0.8134 - val_accuracy: 0.6769\n",
      "Epoch 3/50\n",
      "32/32 - 0s - loss: 0.0887 - accuracy: 1.0000 - val_loss: 0.8132 - val_accuracy: 0.6755\n",
      "Epoch 4/50\n",
      "32/32 - 0s - loss: 0.0874 - accuracy: 1.0000 - val_loss: 0.8109 - val_accuracy: 0.6814\n",
      "Epoch 5/50\n",
      "32/32 - 0s - loss: 0.0863 - accuracy: 1.0000 - val_loss: 0.8120 - val_accuracy: 0.6799\n",
      "Epoch 6/50\n",
      "32/32 - 0s - loss: 0.0853 - accuracy: 1.0000 - val_loss: 0.8128 - val_accuracy: 0.6740\n",
      "Epoch 7/50\n",
      "32/32 - 0s - loss: 0.0845 - accuracy: 1.0000 - val_loss: 0.8124 - val_accuracy: 0.6828\n",
      "Epoch 8/50\n",
      "32/32 - 0s - loss: 0.0835 - accuracy: 1.0000 - val_loss: 0.8140 - val_accuracy: 0.6725\n",
      "Epoch 9/50\n",
      "32/32 - 0s - loss: 0.0824 - accuracy: 1.0000 - val_loss: 0.8230 - val_accuracy: 0.6696\n",
      "Epoch 10/50\n",
      "32/32 - 0s - loss: 0.0817 - accuracy: 1.0000 - val_loss: 0.8147 - val_accuracy: 0.6725\n",
      "Epoch 11/50\n",
      "32/32 - 0s - loss: 0.0809 - accuracy: 1.0000 - val_loss: 0.8184 - val_accuracy: 0.6681\n",
      "Epoch 12/50\n",
      "32/32 - 0s - loss: 0.0799 - accuracy: 1.0000 - val_loss: 0.8150 - val_accuracy: 0.6755\n",
      "Epoch 13/50\n",
      "32/32 - 0s - loss: 0.0789 - accuracy: 1.0000 - val_loss: 0.8154 - val_accuracy: 0.6740\n",
      "Epoch 14/50\n",
      "32/32 - 0s - loss: 0.0784 - accuracy: 1.0000 - val_loss: 0.8162 - val_accuracy: 0.6755\n",
      "Epoch 15/50\n",
      "32/32 - 0s - loss: 0.0773 - accuracy: 1.0000 - val_loss: 0.8146 - val_accuracy: 0.6769\n",
      "Epoch 16/50\n",
      "32/32 - 0s - loss: 0.0769 - accuracy: 1.0000 - val_loss: 0.8148 - val_accuracy: 0.6769\n",
      "Epoch 17/50\n",
      "32/32 - 0s - loss: 0.0757 - accuracy: 1.0000 - val_loss: 0.8147 - val_accuracy: 0.6740\n",
      "Epoch 18/50\n",
      "32/32 - 0s - loss: 0.0751 - accuracy: 1.0000 - val_loss: 0.8155 - val_accuracy: 0.6784\n",
      "Epoch 19/50\n",
      "32/32 - 0s - loss: 0.0744 - accuracy: 1.0000 - val_loss: 0.8196 - val_accuracy: 0.6755\n",
      "Epoch 20/50\n",
      "32/32 - 0s - loss: 0.0737 - accuracy: 1.0000 - val_loss: 0.8171 - val_accuracy: 0.6681\n",
      "Epoch 21/50\n",
      "32/32 - 0s - loss: 0.0729 - accuracy: 1.0000 - val_loss: 0.8171 - val_accuracy: 0.6799\n",
      "Epoch 22/50\n",
      "32/32 - 0s - loss: 0.0721 - accuracy: 1.0000 - val_loss: 0.8265 - val_accuracy: 0.6681\n",
      "Epoch 23/50\n",
      "32/32 - 0s - loss: 0.0716 - accuracy: 1.0000 - val_loss: 0.8228 - val_accuracy: 0.6681\n",
      "Epoch 24/50\n",
      "32/32 - 0s - loss: 0.0708 - accuracy: 1.0000 - val_loss: 0.8187 - val_accuracy: 0.6755\n",
      "Epoch 25/50\n",
      "32/32 - 0s - loss: 0.0699 - accuracy: 1.0000 - val_loss: 0.8188 - val_accuracy: 0.6755\n",
      "Epoch 26/50\n",
      "32/32 - 0s - loss: 0.0691 - accuracy: 1.0000 - val_loss: 0.8193 - val_accuracy: 0.6755\n",
      "Epoch 27/50\n",
      "32/32 - 0s - loss: 0.0689 - accuracy: 1.0000 - val_loss: 0.8185 - val_accuracy: 0.6784\n",
      "Epoch 28/50\n",
      "32/32 - 0s - loss: 0.0679 - accuracy: 1.0000 - val_loss: 0.8188 - val_accuracy: 0.6681\n",
      "Epoch 29/50\n",
      "32/32 - 0s - loss: 0.0672 - accuracy: 1.0000 - val_loss: 0.8200 - val_accuracy: 0.6725\n",
      "Epoch 30/50\n",
      "32/32 - 0s - loss: 0.0666 - accuracy: 1.0000 - val_loss: 0.8232 - val_accuracy: 0.6667\n",
      "Epoch 31/50\n",
      "32/32 - 0s - loss: 0.0661 - accuracy: 1.0000 - val_loss: 0.8188 - val_accuracy: 0.6784\n",
      "Epoch 32/50\n",
      "32/32 - 0s - loss: 0.0654 - accuracy: 1.0000 - val_loss: 0.8194 - val_accuracy: 0.6725\n",
      "Epoch 33/50\n",
      "32/32 - 0s - loss: 0.0650 - accuracy: 1.0000 - val_loss: 0.8204 - val_accuracy: 0.6711\n",
      "Epoch 34/50\n",
      "32/32 - 0s - loss: 0.0643 - accuracy: 1.0000 - val_loss: 0.8238 - val_accuracy: 0.6652\n",
      "Epoch 35/50\n",
      "32/32 - 0s - loss: 0.0638 - accuracy: 1.0000 - val_loss: 0.8237 - val_accuracy: 0.6681\n",
      "Epoch 36/50\n",
      "32/32 - 0s - loss: 0.0630 - accuracy: 1.0000 - val_loss: 0.8213 - val_accuracy: 0.6755\n",
      "Epoch 37/50\n",
      "32/32 - 0s - loss: 0.0626 - accuracy: 1.0000 - val_loss: 0.8253 - val_accuracy: 0.6637\n",
      "Epoch 38/50\n",
      "32/32 - 0s - loss: 0.0618 - accuracy: 1.0000 - val_loss: 0.8215 - val_accuracy: 0.6696\n",
      "Epoch 39/50\n",
      "32/32 - 0s - loss: 0.0613 - accuracy: 1.0000 - val_loss: 0.8215 - val_accuracy: 0.6755\n",
      "Epoch 40/50\n",
      "32/32 - 0s - loss: 0.0609 - accuracy: 1.0000 - val_loss: 0.8224 - val_accuracy: 0.6740\n",
      "Epoch 41/50\n",
      "32/32 - 0s - loss: 0.0603 - accuracy: 1.0000 - val_loss: 0.8228 - val_accuracy: 0.6725\n",
      "Epoch 42/50\n",
      "32/32 - 0s - loss: 0.0601 - accuracy: 1.0000 - val_loss: 0.8259 - val_accuracy: 0.6667\n",
      "Epoch 43/50\n",
      "32/32 - 0s - loss: 0.0591 - accuracy: 1.0000 - val_loss: 0.8240 - val_accuracy: 0.6740\n",
      "Epoch 44/50\n",
      "32/32 - 0s - loss: 0.0587 - accuracy: 1.0000 - val_loss: 0.8244 - val_accuracy: 0.6681\n",
      "Epoch 45/50\n",
      "32/32 - 0s - loss: 0.0581 - accuracy: 1.0000 - val_loss: 0.8243 - val_accuracy: 0.6711\n",
      "Epoch 46/50\n",
      "32/32 - 0s - loss: 0.0576 - accuracy: 1.0000 - val_loss: 0.8257 - val_accuracy: 0.6667\n",
      "Epoch 47/50\n",
      "32/32 - 0s - loss: 0.0577 - accuracy: 1.0000 - val_loss: 0.8261 - val_accuracy: 0.6711\n",
      "Epoch 48/50\n",
      "32/32 - 0s - loss: 0.0565 - accuracy: 1.0000 - val_loss: 0.8265 - val_accuracy: 0.6681\n",
      "Epoch 49/50\n",
      "32/32 - 0s - loss: 0.0563 - accuracy: 1.0000 - val_loss: 0.8272 - val_accuracy: 0.6696\n",
      "Epoch 50/50\n",
      "32/32 - 0s - loss: 0.0556 - accuracy: 1.0000 - val_loss: 0.8288 - val_accuracy: 0.6681\n"
     ]
    }
   ],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "RQ_history = RQ_model.fit(RQ_train_padded, blRQ_train, epochs = 50, \n",
    "                            validation_data = (RQ_test_padded, blRQ_test), verbose = 2, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-244fdfd087851324\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-244fdfd087851324\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_classifier(txt):\n",
    "    text = clean_text(txt)\n",
    "    input_sequence = Type_tok.texts_to_sequences([text])\n",
    "    input_padded = pad_sequences(input_sequence, maxlen=max_length, padding = 'post', truncating = 'post')\n",
    "    \n",
    "    if text == \"\": return\n",
    "    \n",
    "    prediction_list = Type_model.predict(input_padded)\n",
    "    prediction_location = np.argmax(prediction_list)\n",
    "    \n",
    "    type_list = ['general', 'hyperbole', 'rhetorical']\n",
    "\n",
    "    return(type_list[prediction_location])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'general'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_classifier(\"That man is as tall as a house.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gthreshold = 0.7\n",
    "Hthreshold = 0.7\n",
    "Rthreshold = 0.7\n",
    "\n",
    "def sarcasm_classifier(txt, Type):\n",
    "    text = clean_text(txt)\n",
    "    if(Type == None):\n",
    "        print(\"No input text was found\")\n",
    "        return\n",
    "        \n",
    "    elif(Type == 'general'):\n",
    "        input_sequence = GENtok.texts_to_sequences([text])\n",
    "        input_padded = pad_sequences(input_sequence, maxlen=max_length,\n",
    "                                     padding = 'post', truncating = 'post')\n",
    "\n",
    "        predicted_probability = GEN_model.predict(input_padded)\n",
    "    \n",
    "        if(predicted_probability <= Gthreshold):\n",
    "            prediction = \"not sarcastic\"\n",
    "        elif(predicted_probability > Gthreshold):\n",
    "            prediction = \"sarcastic\"\n",
    "        \n",
    "    elif(Type == 'hyperbole'):\n",
    "        input_sequence = HYPtok.texts_to_sequences([text])\n",
    "        input_padded = pad_sequences(input_sequence, maxlen=max_length, padding = 'post', truncating = 'post')\n",
    "\n",
    "        predicted_probability = HYP_model.predict(input_padded)\n",
    "    \n",
    "\n",
    "        if(predicted_probability <= Hthreshold):\n",
    "            prediction = \"not sarcastic\"\n",
    "        elif(predicted_probability > Hthreshold):\n",
    "            prediction = \"sarcastic\"\n",
    "            \n",
    "    elif(Type == 'rhetorical'):\n",
    "        input_sequence = RQtok.texts_to_sequences([text])\n",
    "        input_padded = pad_sequences(input_sequence, maxlen=max_length, padding = 'post', truncating = 'post')\n",
    "\n",
    "        predicted_probability = RQ_model.predict(input_padded)\n",
    "    \n",
    "        if(predicted_probability <= Rthreshold):\n",
    "            prediction = \"not sarcastic\"\n",
    "        elif(predicted_probability > Rthreshold):\n",
    "            prediction = \"sarcastic\"\n",
    "            \n",
    "    print(\"INPUT: \" + text)\n",
    "    print(\"Type: \" + Type + \" statement\")\n",
    "    print(\"prediction: \" + prediction)\n",
    "    print(str(predicted_probability[0][0]) + \" activation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kappa(text):\n",
    "    return sarcasm_classifier(text, type_classifier(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: wow you are good at that\n",
      "Type: hyperbole statement\n",
      "prediction: sarcastic\n",
      "0.70847285 activation\n"
     ]
    }
   ],
   "source": [
    "Kappa(\"wow you're good at that\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update1(df, word, column):\n",
    "    loc = df.loc[df['Words']==word]\n",
    "    locI = int(loc.index.tolist()[0])\n",
    "    new_value = int(loc[column]) + 1\n",
    "    new_v_series = pd.Series([new_value], name=column, index=[locI])\n",
    "    df.update(new_v_series)\n",
    "    return df\n",
    "\n",
    "def binDecode(x, thres):\n",
    "    if (x < thres):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def Count_Correct(model, test_data, test_padded, test_label, word_index, thres):\n",
    "    predicted_labels = model.predict(test_padded)\n",
    "    words = list(word_index.keys())\n",
    "    \n",
    "    df = pd.DataFrame(columns = ['Words', 'Correct', 'Total', 'Rightly Sarcastic'])\n",
    "    df['Words'] = words\n",
    "    df['Correct'] = [0]*len(word_index)\n",
    "    df['Total'] = [0]*len(word_index)\n",
    "    df['Rightly Sarcastic'] = [0]*len(word_index)\n",
    "    df['Incorrectly Sarcastic'] = [0]*len(word_index)\n",
    "    \n",
    "    for i in range(len(test_data)):\n",
    "        boo = (binDecode(predicted_labels[i], thres) == binDecode(test_label[i], thres))\n",
    "        element = test_data[i]\n",
    "        for word in element.split():\n",
    "            if (word in (df.Words.values)):\n",
    "                word = word\n",
    "            else: word = \"<OOV>\"\n",
    "            df = update1(df, word, 'Total')\n",
    "            if boo:\n",
    "                df = update1(df, word, 'Correct')\n",
    "                if (binDecode(predicted_labels[i], thres) == 1):\n",
    "                    df = update1(df, word, 'Rightly Sarcastic')\n",
    "            else:\n",
    "                if (binDecode(predicted_labels[i], thres) == 1):\n",
    "                    df = update1(df, word, 'Incorrectly Sarcastic')\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GENdf = Count_Correct(GEN_model, GEN_test, GEN_test_padded, blGEN_test, GEN_word_index, Gthreshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPdf = Count_Correct(HYP_model, HYP_test, HYP_test_padded, blHYP_test, HYP_word_index, Hthreshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RQdf = Count_Correct(RQ_model, RQ_test, RQ_test_padded, blRQ_test, RQ_word_index, Rthreshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'STOP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-ee4decdca3b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mSTOP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'STOP' is not defined"
     ]
    }
   ],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chdir(cwd)\n",
    "if (os.path.exists(cwd+\"/ModelsSARC\")):\n",
    "    print(\"File exists\")\n",
    "else:\n",
    "    makedirs(\"ModelsSARC\")\n",
    "    chdir(cwd + \"/ModelsSARC\")\n",
    "\n",
    "    Type_model.save('Type_model.h5')\n",
    "    GEN_model.save('GEN_model.h5')\n",
    "    HYP_model.save('HYP_model.h5')\n",
    "    RQ_model.save('RQ_model.h5')\n",
    "\n",
    "    with open('Type_tok.pickle', 'wb') as handle:\n",
    "        pickle.dump(GENtok, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('GENtok.pickle', 'wb') as handle:\n",
    "        pickle.dump(GENtok, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('HYPtok.pickle', 'wb') as handle:\n",
    "        pickle.dump(HYPtok, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('RQtok.pickle', 'wb') as handle:\n",
    "        pickle.dump(RQtok, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chdir(cwd)\n",
    "if (os.path.exists(cwd+\"/dfSARC\")):\n",
    "    print(\"File exists\")\n",
    "else:\n",
    "    makedirs(\"dfSARC\")\n",
    "    chdir(cwd + \"/dfSARC\")\n",
    "    \n",
    "    GENdf.to_csv(\"GENdf.csv\")\n",
    "    HYPdf.to_csv(\"HYPdf.csv\")\n",
    "    RQdf.to_csv(\"RQdf.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENweights = GEN_model.get_layer('GENembed').get_weights()[0]\n",
    "HYPweights = HYP_model.get_layer('HYPembed').get_weights()[0]\n",
    "RQweights = RQ_model.get_layer('RQembed').get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEN_vocab = GEN_word_index\n",
    "Gout_v = io.open('Gvecs.tsv', 'w', encoding='utf-8')\n",
    "Gout_m = io.open('Gmeta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for num, word in enumerate(GEN_vocab):\n",
    "    if num == 0: continue\n",
    "    vec = GENweights[num]\n",
    "    Gout_m.write(word + \"\\n\")\n",
    "    Gout_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "Gout_v.close()\n",
    "Gout_m.close()\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "except ImportError:\n",
    "    pass\n",
    "else:\n",
    "    files.download('Gvecs.tsv')\n",
    "    files.download('Gmeta.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYP_vocab = HYP_word_index\n",
    "Hout_v = io.open('Hvecs.tsv', 'w', encoding='utf-8')\n",
    "Hout_m = io.open('Hmeta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for num, word in enumerate(HYP_vocab):\n",
    "    if num == 0: continue\n",
    "    vec = HYPweights[num]\n",
    "    Hout_m.write(word + \"\\n\")\n",
    "    Hout_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "Hout_v.close()\n",
    "Hout_m.close()\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "except ImportError:\n",
    "    pass\n",
    "else:\n",
    "    files.download('Hvecs.tsv')\n",
    "    files.download('Hmeta.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RQ_vocab = RQ_word_index\n",
    "Rout_v = io.open('Rvecs.tsv', 'w', encoding='utf-8')\n",
    "Rout_m = io.open('Rmeta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for num, word in enumerate(RQ_vocab):\n",
    "    if num == 0: continue\n",
    "    vec = RQweights[num]\n",
    "    Rout_m.write(word + \"\\n\")\n",
    "    Rout_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "Rout_v.close()\n",
    "Rout_m.close()\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "except ImportError:\n",
    "    pass\n",
    "else:\n",
    "    files.download('Rvecs.tsv')\n",
    "    files.download('Rmeta.tsv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
